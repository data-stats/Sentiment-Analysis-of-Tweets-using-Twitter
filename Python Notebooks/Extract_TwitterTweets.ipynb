{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Extract_TwitterTweets.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_E_hBKI-GyD",
        "colab_type": "text"
      },
      "source": [
        "Extracting 3000 tweets from Twitter using API credentials. Hashtags used: #Corona #BJP #Congress"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbfH7yuvL2J9",
        "colab_type": "code",
        "outputId": "78d74b37-2502-4eed-b7f4-d22684ecfe33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!pip install preprocessor"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting preprocessor\n",
            "  Downloading https://files.pythonhosted.org/packages/96/ad/d9f4ffb9bb97d1cb5bcb876b7932571d4dbaa3eff1701ad45d367f0ea27b/preprocessor-1.1.3.tar.gz\n",
            "Building wheels for collected packages: preprocessor\n",
            "  Building wheel for preprocessor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for preprocessor: filename=preprocessor-1.1.3-cp36-none-any.whl size=4478 sha256=568a06befde48613cf93888fc83a5d4fde4ce39ca5b4c27111499813c8d37338\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/c1/a2/21fbcfd80d76576bbf148991a66f00730f541f265c7600000f\n",
            "Successfully built preprocessor\n",
            "Installing collected packages: preprocessor\n",
            "Successfully installed preprocessor-1.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1YLyqZ6L_cj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tweepy import Stream\n",
        "from tweepy import OAuthHandler\n",
        "from tweepy.streaming import StreamListener\n",
        "import json\n",
        "import pandas as pd\n",
        "import csv\n",
        "import re #regular expression\n",
        "from textblob import TextBlob\n",
        "import string\n",
        "import preprocessor as p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_4sXYvwM3qj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "consumer_key = 'b8R1IqFWumzNa3xcyHbo4zFxD'\n",
        "consumer_secret = '5GzK94Lowdnc4W9o8MW8yCdpqUnsc53VfFufck7D7lYNoZq7G7'\n",
        "access_key = '1032017296729038854-uXUp9KF8VEq5ZWYe8LFQkwKT8c336E'\n",
        "access_secret = 'dUHTXKUHro2vdiHsteQyHwSZfegyhpjlxRjsxQ83NvlCb'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ful-mv_INcHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tweepy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9MauaiINRTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pass twitter credentials to tweepy\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_key, access_secret)\n",
        "api = tweepy.API(auth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvucgC-XNVe3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#declare file paths as follows for three files\n",
        "corona_tweets = \"data/corona_data_extraction/corona_data.csv\"\n",
        "bjp_tweets = \"data/bjp_data_extraction/bjp_data.csv\"\n",
        "congress_tweets = \"data/congress_data_extraction/congress_data.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCw_hMt1ORfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#columns of the csv file\n",
        "COLS = ['id', 'created_at', 'source', 'original_text','clean_text', 'sentiment','polarity','subjectivity', 'lang',\n",
        "'favorite_count', 'retweet_count', 'original_author',   'possibly_sensitive', 'hashtags',\n",
        "'user_mentions', 'place', 'place_coord_boundaries']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7HG4ps_OdSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#HappyEmoticons\n",
        "emoticons_happy = set([\n",
        "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
        "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
        "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
        "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
        "    '<3'\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFARS76pOlKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sad Emoticons\n",
        "emoticons_sad = set([\n",
        "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
        "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
        "    ':c', ':{', '>:\\\\', ';('\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfIr3f1nOpGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Emoji patterns\n",
        "emoji_pattern = re.compile(\"[\"\n",
        "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "         u\"\\U00002702-\\U000027B0\"\n",
        "         u\"\\U000024C2-\\U0001F251\"\n",
        "         \"]+\", flags=re.UNICODE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpTf02vwOtC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#combine sad and happy emoticons\n",
        "emoticons = emoticons_happy.union(emoticons_sad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CUnqurgPeeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import preprocessor as p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgSf_Ze6PvmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_tweets(tweet):\n",
        " \n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(tweet)\n",
        "#after tweepy preprocessing the colon symbol left remain after      #removing mentions\n",
        "    tweet = re.sub(r':', '', tweet)\n",
        "    tweet = re.sub(r'‚Ä¶', '', tweet)\n",
        "#replace consecutive non-ASCII characters with a space\n",
        "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
        "#remove emojis from tweet\n",
        "    tweet = emoji_pattern.sub(r'', tweet)\n",
        "#filter using NLTK library append it to a string\n",
        "    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
        "    filtered_tweet = []\n",
        "#looping through conditions\n",
        "    for w in word_tokens:\n",
        "#check tokens against stop words , emoticons and punctuations\n",
        "        if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
        "            filtered_tweet.append(w)\n",
        "    return ' '.join(filtered_tweet)\n",
        "    #print(word_tokens)\n",
        "    #print(filtered_sentence)return tweet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtNNTggYPx3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_tweets(keyword, file):\n",
        "    #If the file exists, then read the existing data from the CSV file.\n",
        "    if os.path.exists(file):\n",
        "        df = pd.read_csv(file, header=0)\n",
        "    else:\n",
        "        df = pd.DataFrame(columns=COLS)\n",
        "    #page attribute in tweepy.cursor and iteration\n",
        "    for page in tweepy.Cursor(api.search, q=keyword,\n",
        "                              count=200, include_rts=False):\n",
        "        for status in page:\n",
        "          new_entry = []\n",
        "          status = status._json\n",
        "        if status['lang'] != 'en':\n",
        "           continue\n",
        "        if status[‘created_at’] in df[‘created_at’].values:\n",
        "           i = df.loc[df[‘created_at’] == status[‘created_at’]].index[0]\n",
        "        if status[‘favorite_count’] != df.at[i, ‘favorite_count’] or \\\n",
        "            status[‘retweet_count’] != df.at[i, ‘retweet_count’]:\n",
        "            df.at[i, ‘favorite_count’] = status[‘favorite_count’]\n",
        "            df.at[i, ‘retweet_count’] = status[‘retweet_count’]\n",
        "            continue"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}